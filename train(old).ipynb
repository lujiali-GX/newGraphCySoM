{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0258359",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "import torch\n",
    "from torch_geometric.data import Dataset, Data, DataLoader\n",
    "from torch_geometric import loader\n",
    "import numpy as np\n",
    "import os\n",
    "import networkx as nx\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.nn import MFConv\n",
    "from torch.utils.data import random_split\n",
    "import pickle\n",
    "from torch_geometric.utils import from_networkx\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score, precision_score, f1_score, recall_score, jaccard_score, balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8a08933",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datapath = './datasets/train_datsets.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dc9c371",
   "metadata": {},
   "outputs": [],
   "source": [
    "fr = open(train_datapath, 'rb')\n",
    "train_data = pickle.load(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "900082e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datapath = './datasets/test_datsets.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4b55eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fe = open(test_datapath, 'rb')\n",
    "test_data = pickle.load(fe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fda65fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(544, 136)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data),len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abab68c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Model, self).__init__()\n",
    "        num_classses = 2\n",
    "\n",
    "        conv_hidden = args['conv_hidden']\n",
    "        cls_hidden = args['cls_hidden']\n",
    "        self.n_layers = args['n_layers']\n",
    "        # cls_drop = ['cls_drop']\n",
    "\n",
    "#         self.conv_layers = nn.ModuleList([])\n",
    "\n",
    "        self.conv1 = MFConv(29, conv_hidden, 5)\n",
    "\n",
    "#         for i in range(self.n_layers):\n",
    "#             self.conv_layers.append(\n",
    "#                 MFConv(conv_hidden, conv_hidden)\n",
    "#             )\n",
    "        self.conv2 = MFConv(conv_hidden, conv_hidden, 5)\n",
    "        self.conv3 = MFConv(conv_hidden, conv_hidden, 5)\n",
    "        self.conv4 = MFConv(conv_hidden, conv_hidden, 5)\n",
    "\n",
    "        self.linear1 = nn.Linear(conv_hidden, cls_hidden)\n",
    "        self.linear2 = nn.Linear(cls_hidden, num_classses)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop1 = nn.Dropout(p=0.5)\n",
    "\n",
    "    \n",
    "    def forward(self, mol):\n",
    "\n",
    "        res = self.conv1(mol.x, mol.edge_index)\n",
    "#         for i in range(self.n_layers):\n",
    "#             res = self.conv_layers[i](res, mol.edge_index)\n",
    "            \n",
    "        \n",
    "        res = self.conv2(res, mol.edge_index)\n",
    "        res_2 = res\n",
    "        res = self.conv3(res, mol.edge_index)\n",
    "        res_3 = res\n",
    "        res = self.conv4(res, mol.edge_index)\n",
    "        \n",
    "        \n",
    "        res = res+res_3+res_2\n",
    "#         res = torch.cat((res, res_3), dim=-1)\n",
    "          \n",
    "        res = self.linear1(res)\n",
    "        \n",
    "        res = self.relu(res)\n",
    "        # fc relu fc relu\n",
    "        res = self.drop1(res)\n",
    "        res = self.linear2(res)\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10334a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "# 可显示数组中的所有元素，不受默认的截断限制\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "def seed_torch(seed=42):\n",
    "    # 设置种子可以确保每次运行代码时生成的随机数相同。\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed) \n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a2c93e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "def top2(output, label):\n",
    "    sf = nn.Softmax(dim=1)\n",
    "    preds = sf(output)\n",
    "    preds = preds[:, 1]\n",
    "    _, indices = torch.topk(preds, 2)\n",
    "    pos_index = []\n",
    "    for i in range(label.shape[0]):\n",
    "        if label[i] == 1:\n",
    "            pos_index.append(i)  \n",
    "    for li in pos_index:\n",
    "        if li in indices:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def MCC(output, label):\n",
    "    tn,fp,fn,tp=confusion_matrix(label, output).ravel()\n",
    "    up = (tp * tn) - (fp * fn)\n",
    "    down = ((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) ** 0.5\n",
    "    return up / down\n",
    "\n",
    "def metrics(output, label):\n",
    "    tn,fp,fn,tp=confusion_matrix(label, output).ravel()\n",
    "    up = (tp * tn) - (fp * fn)\n",
    "    down = ((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) ** 0.5\n",
    "    mcc = up / down\n",
    "    selectivity = tn / (tn + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    g_mean = (selectivity * recall) ** 0.5\n",
    "    balancedAccuracy = (recall + selectivity) / 2\n",
    "    return mcc, selectivity, recall, g_mean, balancedAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3f482af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Imbalanced_ratio(dataset_loader):\n",
    "    som = 0\n",
    "    no_som = 0\n",
    "    for i in dataset_loader:\n",
    "        for n in i.y:\n",
    "            if n == 0:\n",
    "                no_som = no_som+1\n",
    "            else:\n",
    "                som = som+1\n",
    "    return('SOM vs no_SOM:', som, no_som,'1:{}'.format(no_som/som))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08fbeb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, training_set, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    sf = nn.Softmax(dim=1)\n",
    "    total_loss = 0\n",
    "    all_pred = []\n",
    "    all_pred_raw = []\n",
    "    all_labels = []\n",
    "    top2n = 0\n",
    "    for mol in training_set:\n",
    "        mol = mol.to(device)\n",
    "        mol.x = mol.x.to(torch.float32)\n",
    "        target = mol.y\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(mol)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        # tracking\n",
    "        top2n += top2(output, target)\n",
    "        all_pred.append(np.argmax(output.cpu().detach().numpy(), axis=1))\n",
    "        all_pred_raw.append(sf(output)[:, 1].cpu().detach().numpy())\n",
    "        all_labels.append(target.cpu().detach().numpy())\n",
    "\n",
    "    all_pred = np.concatenate(all_pred).ravel()\n",
    "    all_pred_raw = np.concatenate(all_pred_raw).ravel()\n",
    "    all_labels = np.concatenate(all_labels).ravel()\n",
    "\n",
    "    mcc = MCC(all_pred, all_labels)\n",
    "    print(f'Train Epoch: {epoch}, Ave Loss: {total_loss / len(training_set)} ACC: {accuracy_score(all_labels, all_pred)} Top2: {top2n / len(training_set)} AUC: {roc_auc_score(all_labels, all_pred_raw)} MCC: {mcc}')\n",
    "    return top2n / len(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d434da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(args, model, device, val_set, optimizer, criterion, epoch):\n",
    "    model.eval()\n",
    "    sf = nn.Softmax(dim=1)\n",
    "    total_loss = 0\n",
    "    all_pred = []\n",
    "    all_pred_raw = []\n",
    "    all_labels = []\n",
    "    top2n = 0\n",
    "    for mol in val_set:\n",
    "        mol = mol.to(device)\n",
    "        mol.x = mol.x.to(torch.float32)\n",
    "        target = mol.y\n",
    "        optimizer.zero_grad()\n",
    "        output = model(mol)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        # tracking\n",
    "        top2n += top2(output, target)\n",
    "        all_pred.append(np.argmax(output.cpu().detach().numpy(), axis=1))\n",
    "        all_pred_raw.append(sf(output)[:, 1].cpu().detach().numpy())\n",
    "        all_labels.append(target.cpu().detach().numpy())\n",
    "    all_pred = np.concatenate(all_pred).ravel()\n",
    "    all_pred_raw = np.concatenate(all_pred_raw).ravel()\n",
    "    all_labels = np.concatenate(all_labels).ravel()\n",
    "    mcc = MCC(all_pred, all_labels)\n",
    "\n",
    "    print(f'Val Epoch: {epoch}, Ave Loss: {total_loss / len(val_set)} ACC: {accuracy_score(all_labels, all_pred)} Top2: {top2n / len(val_set)} AUC: {roc_auc_score(all_labels, all_pred_raw)} MCC: {mcc}')\n",
    "    return top2n / len(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f33f3815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_set):\n",
    "    model.eval()\n",
    "    sf = nn.Softmax(dim=1)\n",
    "    all_pred = []\n",
    "    all_pred_raw = []\n",
    "    all_labels = []\n",
    "    top2n = 0\n",
    "    with torch.no_grad():\n",
    "        for mol in test_set:\n",
    "            mol = mol.to(device)\n",
    "            mol.x = mol.x.to(torch.float32)\n",
    "            mol.edge_attr = mol.edge_attr.to(torch.float32)\n",
    "            target = mol.y\n",
    "            output = model(mol)\n",
    "            # squeeze\n",
    "            output = torch.squeeze(output)\n",
    "            # tracking\n",
    "            top2n += top2(output, target)\n",
    "            all_pred.append(np.argmax(output.cpu().detach().numpy(), axis=1))\n",
    "            all_pred_raw.append(sf(output)[:, 1].cpu().detach().numpy())\n",
    "            all_labels.append(target.cpu().detach().numpy())\n",
    "    all_pred = np.concatenate(all_pred).ravel()\n",
    "    all_pred_raw = np.concatenate(all_pred_raw).ravel()\n",
    "    all_labels = np.concatenate(all_labels).ravel()\n",
    "    mcc, selectivity, recall, g_mean, balancedAcc = metrics(all_pred, all_labels)\n",
    "    print(f'ACC: {accuracy_score(all_labels, all_pred)} \\\n",
    "        Top2: {top2n / len(test_set)} \\\n",
    "        AUC: {roc_auc_score(all_labels, all_pred_raw)}\\\n",
    "        MCC: {mcc} selectivity {selectivity} recall {recall} \\\n",
    "        g_mean {g_mean} balanced acc {balancedAcc} f1score {f1_score(all_labels, all_pred)} \\\n",
    "        precision score {precision_score(all_labels, all_pred)} jaccard score {jaccard_score(all_labels, all_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c3ac956",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set, validation_set = random_split(train_data, [int(len(train_data) * 0.85), len(train_data) - int(len(train_data) * 0.85)], generator=torch.Generator().manual_seed(12345))\n",
    "batch_size = 1\n",
    "train_loader = loader.DataLoader(training_set, batch_size, shuffle=True)\n",
    "val_loader = loader.DataLoader(validation_set, batch_size, shuffle=True)\n",
    "test_loader = loader.DataLoader(test_data, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8c26a57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(462, 82, 136)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader), len(val_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd196296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args, train_loader, val_loader):\n",
    "    seed_torch(args['seed'])\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    torch.manual_seed(args['seed'])\n",
    "\n",
    "    model = Model(args).to(device)\n",
    "    print(model)\n",
    "    weights = torch.tensor([1, args['pos_weight']], dtype=torch.float32).to(device)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(weight=weights)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=args['lr'])\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    max_top2 = 0\n",
    "    for epoch in range(1, args['epoch'] + 1):\n",
    "        train(args, model, device, train_loader, optimizer, loss_fn, epoch)\n",
    "        top2acc = val(args, model, device, val_loader, optimizer, loss_fn, epoch)\n",
    "        scheduler.step()\n",
    "        if top2acc > max_top2:\n",
    "            max_top2 = top2acc\n",
    "            print('Saving model (epoch = {:4d}, top2acc = {:.4f})'\n",
    "                .format(epoch, max_top2))\n",
    "            torch.save(model.state_dict(), args['save_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a9d721e",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'lr': 0.01,\n",
    "    'epoch': 400,\n",
    "    'seed': 12345,\n",
    "    'save_path': './model/train_sum',   # 对应修改模型\n",
    "    'pos_weight': 3,\n",
    "    'conv_hidden': 1024, \n",
    "    'cls_hidden': 1024,\n",
    "    'n_layers': 3,\n",
    "    'max_degree': 5\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83beefb6",
   "metadata": {},
   "source": [
    "train_test 最后结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1cd4a11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (conv1): MFConv(29, 1024)\n",
      "  (conv2): MFConv(1024, 1024)\n",
      "  (conv3): MFConv(1024, 1024)\n",
      "  (conv4): MFConv(1024, 1024)\n",
      "  (linear1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (linear2): Linear(in_features=1024, out_features=2, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (drop1): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Train Epoch: 1, Ave Loss: 0.6047556184038714 ACC: 0.8018507918336195 Top2: 0.512987012987013 AUC: 0.719470594755429 MCC: 0.19359717191821119\n",
      "Val Epoch: 1, Ave Loss: 0.49343662072972555 ACC: 0.8546731496488384 Top2: 0.6463414634146342 AUC: 0.7889227948563392 MCC: 0.29700077985815454\n",
      "Saving model (epoch =    1, top2acc = 0.6463)\n",
      "Train Epoch: 2, Ave Loss: 0.47702263553679247 ACC: 0.8499332188513642 Top2: 0.6601731601731602 AUC: 0.7875943621025987 MCC: 0.3004896050531058\n",
      "Val Epoch: 2, Ave Loss: 0.460543882192635 ACC: 0.8606158833063209 Top2: 0.6707317073170732 AUC: 0.8243796463733172 MCC: 0.34544465863283574\n",
      "Saving model (epoch =    2, top2acc = 0.6707)\n",
      "Train Epoch: 3, Ave Loss: 0.44135262374463796 ACC: 0.8581377599694715 Top2: 0.6753246753246753 AUC: 0.8144872281148439 MCC: 0.3224715357789065\n",
      "Val Epoch: 3, Ave Loss: 0.44270652859676174 ACC: 0.8741220961642355 Top2: 0.7195121951219512 AUC: 0.8326489351014668 MCC: 0.3368625365041126\n",
      "Saving model (epoch =    3, top2acc = 0.7195)\n",
      "Train Epoch: 4, Ave Loss: 0.41940969829870894 ACC: 0.8679641289830186 Top2: 0.70995670995671 AUC: 0.8363688064529857 MCC: 0.3621838382219518\n",
      "Val Epoch: 4, Ave Loss: 0.40682465355934166 ACC: 0.8914100486223663 Top2: 0.7439024390243902 AUC: 0.8595413903958208 MCC: 0.41193705758425375\n",
      "Saving model (epoch =    4, top2acc = 0.7439)\n",
      "Train Epoch: 5, Ave Loss: 0.40009839935389496 ACC: 0.8729250143102462 Top2: 0.7597402597402597 AUC: 0.8512408490800325 MCC: 0.3876623076357905\n",
      "Val Epoch: 5, Ave Loss: 0.40974154121174317 ACC: 0.8757428417071853 Top2: 0.7439024390243902 AUC: 0.8535890094434397 MCC: 0.3874977010241377\n",
      "Train Epoch: 6, Ave Loss: 0.39325056320612695 ACC: 0.8732112192329708 Top2: 0.7597402597402597 AUC: 0.8605649776701885 MCC: 0.39115199879002555\n",
      "Val Epoch: 6, Ave Loss: 0.3946122503953009 ACC: 0.8941112911939492 Top2: 0.7317073170731707 AUC: 0.870215491259795 MCC: 0.4119043244181587\n",
      "Train Epoch: 7, Ave Loss: 0.37897276336093355 ACC: 0.8705399732875405 Top2: 0.79004329004329 AUC: 0.8705994323190873 MCC: 0.3979320817458737\n",
      "Val Epoch: 7, Ave Loss: 0.3675030057386654 ACC: 0.882766072393301 Top2: 0.7804878048780488 AUC: 0.883093480008037 MCC: 0.42106927412406364\n",
      "Saving model (epoch =    7, top2acc = 0.7805)\n",
      "Train Epoch: 8, Ave Loss: 0.3654759658694074 ACC: 0.8742606372829612 Top2: 0.7943722943722944 AUC: 0.878238758042497 MCC: 0.41203691064259207\n",
      "Val Epoch: 8, Ave Loss: 0.35819177658910434 ACC: 0.9000540248514317 Top2: 0.7560975609756098 AUC: 0.8924207604982922 MCC: 0.468639612177602\n",
      "Train Epoch: 9, Ave Loss: 0.3557758448621282 ACC: 0.881415760351078 Top2: 0.8116883116883117 AUC: 0.8867165566116492 MCC: 0.43954014141984776\n",
      "Val Epoch: 9, Ave Loss: 0.3464372848592153 ACC: 0.8908698001080497 Top2: 0.8048780487804879 AUC: 0.8964580821780189 MCC: 0.44737140740193765\n",
      "Saving model (epoch =    9, top2acc = 0.8049)\n",
      "Train Epoch: 10, Ave Loss: 0.34113487052213143 ACC: 0.8823697767601603 Top2: 0.829004329004329 AUC: 0.895489934631108 MCC: 0.46125648150196696\n",
      "Val Epoch: 10, Ave Loss: 0.33265272909548227 ACC: 0.8914100486223663 Top2: 0.7804878048780488 AUC: 0.9046959764918625 MCC: 0.4514024603911221\n",
      "Train Epoch: 11, Ave Loss: 0.3314413107383303 ACC: 0.8821789734783438 Top2: 0.8311688311688312 AUC: 0.901661278109863 MCC: 0.4578113693283135\n",
      "Val Epoch: 11, Ave Loss: 0.3094235739284536 ACC: 0.9000540248514317 Top2: 0.8414634146341463 AUC: 0.9205752712477396 MCC: 0.5165820794815188\n",
      "Saving model (epoch =   11, top2acc = 0.8415)\n",
      "Train Epoch: 12, Ave Loss: 0.31807241763806704 ACC: 0.8895248998282771 Top2: 0.8679653679653679 AUC: 0.9082067727915374 MCC: 0.4927261577995684\n",
      "Val Epoch: 12, Ave Loss: 0.31226850260139966 ACC: 0.8919502971366828 Top2: 0.8292682926829268 AUC: 0.914889742816958 MCC: 0.4903870324564578\n",
      "Train Epoch: 13, Ave Loss: 0.30917782447143616 ACC: 0.8910513260828086 Top2: 0.8333333333333334 AUC: 0.9148385093254849 MCC: 0.5062648342216618\n",
      "Val Epoch: 13, Ave Loss: 0.29254531349259905 ACC: 0.902215018908698 Top2: 0.8414634146341463 AUC: 0.9260441782198112 MCC: 0.5568186170867052\n",
      "Train Epoch: 14, Ave Loss: 0.30242363580661824 ACC: 0.8960122114100363 Top2: 0.8593073593073594 AUC: 0.9192572350319643 MCC: 0.5272624728018545\n",
      "Val Epoch: 14, Ave Loss: 0.28843315618067256 ACC: 0.9016747703943814 Top2: 0.8536585365853658 AUC: 0.9271272855133613 MCC: 0.5140756040093196\n",
      "Saving model (epoch =   14, top2acc = 0.8537)\n",
      "Train Epoch: 15, Ave Loss: 0.2957030255549075 ACC: 0.8958214081282198 Top2: 0.8787878787878788 AUC: 0.9210210773403273 MCC: 0.5205471768883339\n",
      "Val Epoch: 15, Ave Loss: 0.278923640118503 ACC: 0.9005942733657483 Top2: 0.8292682926829268 AUC: 0.934272654209363 MCC: 0.5203996928512009\n",
      "Train Epoch: 16, Ave Loss: 0.2912834236721883 ACC: 0.8989696622781912 Top2: 0.8744588744588745 AUC: 0.9253066052185676 MCC: 0.5404041050815751\n",
      "Val Epoch: 16, Ave Loss: 0.2715843099896319 ACC: 0.899513776337115 Top2: 0.8536585365853658 AUC: 0.9393930831826401 MCC: 0.5498589048709113\n",
      "Train Epoch: 17, Ave Loss: 0.2837520171957467 ACC: 0.898015645869109 Top2: 0.8593073593073594 AUC: 0.9298346766141 MCC: 0.5401750686408041\n",
      "Val Epoch: 17, Ave Loss: 0.270360201506353 ACC: 0.9092382495948136 Top2: 0.8902439024390244 AUC: 0.9357324944745831 MCC: 0.5341834486019366\n",
      "Saving model (epoch =   17, top2acc = 0.8902)\n",
      "Train Epoch: 18, Ave Loss: 0.2739658456286153 ACC: 0.9002098836099981 Top2: 0.8809523809523809 AUC: 0.9335975152618613 MCC: 0.5538545044473695\n",
      "Val Epoch: 18, Ave Loss: 0.26522880422360284 ACC: 0.9130199891950297 Top2: 0.8780487804878049 AUC: 0.9436941179425357 MCC: 0.5738663280187994\n",
      "Train Epoch: 19, Ave Loss: 0.27509599006504226 ACC: 0.8996374737645487 Top2: 0.9112554112554112 AUC: 0.9330467646911638 MCC: 0.5474647852313359\n",
      "Val Epoch: 19, Ave Loss: 0.2595184190743944 ACC: 0.9081577525661805 Top2: 0.8902439024390244 AUC: 0.9434398231866588 MCC: 0.5662251490995375\n",
      "Train Epoch: 20, Ave Loss: 0.2678725763054734 ACC: 0.9050753672963175 Top2: 0.8961038961038961 AUC: 0.9368370099145391 MCC: 0.5665547182462284\n",
      "Val Epoch: 20, Ave Loss: 0.24942604264971324 ACC: 0.9168017287952458 Top2: 0.9024390243902439 AUC: 0.9478758539280692 MCC: 0.6181923167965597\n",
      "Saving model (epoch =   20, top2acc = 0.9024)\n",
      "Train Epoch: 21, Ave Loss: 0.26347557993129994 ACC: 0.9040259492463271 Top2: 0.8917748917748918 AUC: 0.9393649625170647 MCC: 0.5622178802446338\n",
      "Val Epoch: 21, Ave Loss: 0.24756684976561769 ACC: 0.9184224743381956 Top2: 0.8902439024390244 AUC: 0.949973000803697 MCC: 0.6087484050470591\n",
      "Train Epoch: 22, Ave Loss: 0.2529611598112665 ACC: 0.9100362526235451 Top2: 0.8852813852813853 AUC: 0.9438554317167039 MCC: 0.5963004961490095\n",
      "Val Epoch: 22, Ave Loss: 0.2317306820939227 ACC: 0.9168017287952458 Top2: 0.9024390243902439 AUC: 0.9550275015069319 MCC: 0.6100246747613263\n",
      "Train Epoch: 23, Ave Loss: 0.2512989047403001 ACC: 0.9085098263690136 Top2: 0.8982683982683982 AUC: 0.943783407610142 MCC: 0.5864407214884814\n",
      "Val Epoch: 23, Ave Loss: 0.23013196365408053 ACC: 0.9216639654240951 Top2: 0.9146341463414634 AUC: 0.9564779234478602 MCC: 0.6265619658595672\n",
      "Saving model (epoch =   23, top2acc = 0.9146)\n",
      "Train Epoch: 24, Ave Loss: 0.245174494073187 ACC: 0.9128029002098836 Top2: 0.9112554112554112 AUC: 0.9468651039929885 MCC: 0.6112680660477842\n",
      "Val Epoch: 24, Ave Loss: 0.23056702116197655 ACC: 0.9173419773095624 Top2: 0.8780487804878049 AUC: 0.9560540988547318 MCC: 0.6156764926731355\n",
      "Train Epoch: 25, Ave Loss: 0.24101697444022246 ACC: 0.9122304903644343 Top2: 0.9307359307359307 AUC: 0.948829397808316 MCC: 0.6045151190594338\n",
      "Val Epoch: 25, Ave Loss: 0.21697822741017017 ACC: 0.9254457050243112 Top2: 0.9024390243902439 AUC: 0.9617929977898332 MCC: 0.6402512042829827\n",
      "Train Epoch: 26, Ave Loss: 0.24263107959195937 ACC: 0.911276473955352 Top2: 0.9155844155844156 AUC: 0.9484094130830387 MCC: 0.6018394270971625\n",
      "Val Epoch: 26, Ave Loss: 0.21557700670914862 ACC: 0.917882225823879 Top2: 0.926829268292683 AUC: 0.9614413803496082 MCC: 0.6132005017297364\n",
      "Saving model (epoch =   26, top2acc = 0.9268)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 27, Ave Loss: 0.23655522608361257 ACC: 0.9144247281053234 Top2: 0.9155844155844156 AUC: 0.9510454486143076 MCC: 0.6229372418383361\n",
      "Val Epoch: 27, Ave Loss: 0.20929131631898443 ACC: 0.9232847109670448 Top2: 0.9024390243902439 AUC: 0.9657235784609203 MCC: 0.6199767095577966\n",
      "Train Epoch: 28, Ave Loss: 0.22592669940553353 ACC: 0.9178591871780195 Top2: 0.935064935064935 AUC: 0.9551595684727102 MCC: 0.6313431934870689\n",
      "Val Epoch: 28, Ave Loss: 0.2045769411531027 ACC: 0.9211237169097785 Top2: 0.9146341463414634 AUC: 0.9645808217801889 MCC: 0.6367493583383844\n",
      "Train Epoch: 29, Ave Loss: 0.22538492520648715 ACC: 0.9198626216370922 Top2: 0.9372294372294372 AUC: 0.9557653192702368 MCC: 0.6475568113948126\n",
      "Val Epoch: 29, Ave Loss: 0.20484822510639433 ACC: 0.9227444624527282 Top2: 0.9512195121951219 AUC: 0.96485709262608 MCC: 0.6512927392154875\n",
      "Saving model (epoch =   29, top2acc = 0.9512)\n",
      "Train Epoch: 30, Ave Loss: 0.22115892062436898 ACC: 0.9203396298416333 Top2: 0.9415584415584416 AUC: 0.9582841987060915 MCC: 0.6466608975246403\n",
      "Val Epoch: 30, Ave Loss: 0.20021465114919787 ACC: 0.9232847109670448 Top2: 0.9146341463414634 AUC: 0.9672493469961825 MCC: 0.6412344944930312\n",
      "Train Epoch: 31, Ave Loss: 0.21169571039288052 ACC: 0.9229154741461553 Top2: 0.9393939393939394 AUC: 0.960892593817245 MCC: 0.6545127129313367\n",
      "Val Epoch: 31, Ave Loss: 0.19554761307393542 ACC: 0.9270664505672609 Top2: 0.9390243902439024 AUC: 0.9684800080369701 MCC: 0.6641320441453221\n",
      "Train Epoch: 32, Ave Loss: 0.21260612336913673 ACC: 0.9251097118870445 Top2: 0.9502164502164502 AUC: 0.9607788518515573 MCC: 0.6644837867838238\n",
      "Val Epoch: 32, Ave Loss: 0.19577274769942118 ACC: 0.9357104267963263 Top2: 0.926829268292683 AUC: 0.9686369801084991 MCC: 0.6898638337773807\n",
      "Train Epoch: 33, Ave Loss: 0.21204242596727316 ACC: 0.9244419004006869 Top2: 0.9458874458874459 AUC: 0.9612663708689614 MCC: 0.6621807741340424\n",
      "Val Epoch: 33, Ave Loss: 0.19489744309585813 ACC: 0.9324689357104268 Top2: 0.9024390243902439 AUC: 0.9677579365079366 MCC: 0.6881307309493244\n",
      "Train Epoch: 34, Ave Loss: 0.2079319470561159 ACC: 0.9225338675825224 Top2: 0.9372294372294372 AUC: 0.9617215258073131 MCC: 0.6575097364301721\n",
      "Val Epoch: 34, Ave Loss: 0.184529902174978 ACC: 0.9357104267963263 Top2: 0.9634146341463414 AUC: 0.9720621107092626 MCC: 0.703656639151365\n",
      "Saving model (epoch =   34, top2acc = 0.9634)\n",
      "Train Epoch: 35, Ave Loss: 0.20371255091338525 ACC: 0.9234878839916046 Top2: 0.9523809523809523 AUC: 0.963567213691315 MCC: 0.6587267206381411\n",
      "Val Epoch: 35, Ave Loss: 0.18163650745253374 ACC: 0.9303079416531604 Top2: 0.9634146341463414 AUC: 0.9730730108499097 MCC: 0.6906045054981824\n",
      "Train Epoch: 36, Ave Loss: 0.20162926357326671 ACC: 0.9242510971188704 Top2: 0.9415584415584416 AUC: 0.9641767060008709 MCC: 0.6634898684707254\n",
      "Val Epoch: 36, Ave Loss: 0.17876552763713024 ACC: 0.942733657482442 Top2: 0.926829268292683 AUC: 0.9748656319067712 MCC: 0.7284331778409243\n",
      "Train Epoch: 37, Ave Loss: 0.19901110940683786 ACC: 0.9265407365006678 Top2: 0.961038961038961 AUC: 0.9651012336232848 MCC: 0.6721621394857045\n",
      "Val Epoch: 37, Ave Loss: 0.1807061006803429 ACC: 0.930848190167477 Top2: 0.9634146341463414 AUC: 0.9728658077154914 MCC: 0.692218782285457\n",
      "Train Epoch: 38, Ave Loss: 0.20000078963279977 ACC: 0.9268269414233925 Top2: 0.9437229437229437 AUC: 0.9653324590666893 MCC: 0.6705788604867875\n",
      "Val Epoch: 38, Ave Loss: 0.18030165420226163 ACC: 0.9416531604538088 Top2: 0.9512195121951219 AUC: 0.9730384769941731 MCC: 0.7428877649493072\n",
      "Train Epoch: 39, Ave Loss: 0.19416104248892194 ACC: 0.9256821217324938 Top2: 0.948051948051948 AUC: 0.9667779922391687 MCC: 0.6702149786532613\n",
      "Val Epoch: 39, Ave Loss: 0.17658754429927595 ACC: 0.9373311723392761 Top2: 0.9512195121951219 AUC: 0.9741341420534458 MCC: 0.7122760202484427\n",
      "Train Epoch: 40, Ave Loss: 0.1953366539321736 ACC: 0.9262545315779431 Top2: 0.961038961038961 AUC: 0.9666286123714032 MCC: 0.6743819683847658\n",
      "Val Epoch: 40, Ave Loss: 0.17327618830916813 ACC: 0.9373311723392761 Top2: 0.926829268292683 AUC: 0.9754652652200121 MCC: 0.7037536324838194\n",
      "Train Epoch: 41, Ave Loss: 0.1910350430086128 ACC: 0.9262545315779431 Top2: 0.9415584415584416 AUC: 0.9671837592187349 MCC: 0.6706899497275249\n",
      "Val Epoch: 41, Ave Loss: 0.17013170555938126 ACC: 0.9389519178822259 Top2: 0.9634146341463414 AUC: 0.9765232569821177 MCC: 0.7327891910865063\n",
      "Train Epoch: 42, Ave Loss: 0.18652661638141785 ACC: 0.9322648349551612 Top2: 0.9567099567099567 AUC: 0.970373304685823 MCC: 0.7026258767384536\n",
      "Val Epoch: 42, Ave Loss: 0.17323942124849276 ACC: 0.9389519178822259 Top2: 0.9512195121951219 AUC: 0.9759393208760297 MCC: 0.6960129113827231\n",
      "Train Epoch: 43, Ave Loss: 0.18399353481961095 ACC: 0.9324556382369776 Top2: 0.9588744588744589 AUC: 0.970632965620649 MCC: 0.6999946501861981\n",
      "Val Epoch: 43, Ave Loss: 0.16653599346442738 ACC: 0.9411129119394922 Top2: 0.9512195121951219 AUC: 0.9771825396825398 MCC: 0.7197094241820929\n",
      "Train Epoch: 44, Ave Loss: 0.1866614544765169 ACC: 0.9299751955733638 Top2: 0.9675324675324676 AUC: 0.9693791849396658 MCC: 0.6882823880733816\n",
      "Val Epoch: 44, Ave Loss: 0.16314755044536802 ACC: 0.9438141545110751 Top2: 0.9634146341463414 AUC: 0.9789814396222624 MCC: 0.7596070499445523\n",
      "Train Epoch: 45, Ave Loss: 0.18194861419816763 ACC: 0.9323602365960695 Top2: 0.9545454545454546 AUC: 0.9709701693922802 MCC: 0.6991183736073922\n",
      "Val Epoch: 45, Ave Loss: 0.16562307586825295 ACC: 0.9438141545110751 Top2: 0.9512195121951219 AUC: 0.9773206751054851 MCC: 0.7497546200476182\n",
      "Train Epoch: 46, Ave Loss: 0.18220922812695758 ACC: 0.9333142530051517 Top2: 0.9567099567099567 AUC: 0.9712137418253809 MCC: 0.7040904011730407\n",
      "Val Epoch: 46, Ave Loss: 0.1602764722178463 ACC: 0.9405726634251755 Top2: 0.9634146341463414 AUC: 0.9782468103275066 MCC: 0.7277759369364262\n",
      "Train Epoch: 47, Ave Loss: 0.17909542048071522 ACC: 0.9314062201869873 Top2: 0.9523809523809523 AUC: 0.9719404369992513 MCC: 0.6976684281115045\n",
      "Val Epoch: 47, Ave Loss: 0.1633731063154367 ACC: 0.9475958941112912 Top2: 0.9512195121951219 AUC: 0.9783566907775769 MCC: 0.7636899499392177\n",
      "Train Epoch: 48, Ave Loss: 0.178232874730486 ACC: 0.9328372448006106 Top2: 0.9523809523809523 AUC: 0.9724527435338486 MCC: 0.7032037711262116\n",
      "Val Epoch: 48, Ave Loss: 0.15900618953505394 ACC: 0.9454349000540249 Top2: 0.9634146341463414 AUC: 0.9792545710267229 MCC: 0.7502343637574864\n",
      "Train Epoch: 49, Ave Loss: 0.17701106009032483 ACC: 0.9299751955733638 Top2: 0.9632034632034632 AUC: 0.9723168331093882 MCC: 0.6933557702093746\n",
      "Val Epoch: 49, Ave Loss: 0.1600604755406427 ACC: 0.9443544030253916 Top2: 0.975609756097561 AUC: 0.9788809774964838 MCC: 0.7482670041387978\n",
      "Saving model (epoch =   49, top2acc = 0.9756)\n",
      "Train Epoch: 50, Ave Loss: 0.17580055476750778 ACC: 0.9332188513642434 Top2: 0.9632034632034632 AUC: 0.9726433735717362 MCC: 0.706696178000219\n",
      "Val Epoch: 50, Ave Loss: 0.1575602388943591 ACC: 0.946515397082658 Top2: 0.9634146341463414 AUC: 0.9795999095840869 MCC: 0.7648904748712803\n"
     ]
    }
   ],
   "source": [
    "main(args, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "510de484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(args).to(\"cuda\")\n",
    "model.load_state_dict(torch.load(args['save_path']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7cfadccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('SOM vs no_SOM:', 1145, 9337, '1:8.154585152838427')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Imbalanced_ratio(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb1da0e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('SOM vs no_SOM:', 192, 1659, '1:8.640625')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Imbalanced_ratio(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93d59220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.224382946896036"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(9337+1659)/(1145+192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd86b6d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('SOM vs no_SOM:', 325, 2611, '1:8.033846153846154')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Imbalanced_ratio(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf2350c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC: 0.8998637602179836         Top2: 0.8823529411764706         AUC: 0.9188627993989925        MCC: 0.573963336635243 selectivity 0.9199540405974722 recall 0.7384615384615385         g_mean 0.8242273206667673 balanced acc 0.8292077895295054 f1score 0.6201550387596899         precision score 0.534521158129176 jaccard score 0.449438202247191\n"
     ]
    }
   ],
   "source": [
    "test(model, \"cuda\", test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4908ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "方差为: 0.000877799999999998\n",
      "标准差为: 0.029627689751311997\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "data = [0.665,\n",
    "0.714,\n",
    "0.668,\n",
    "0.720,\n",
    "0.726]\n",
    "variance = statistics.variance(data)\n",
    "standard_deviation = statistics.stdev(data)\n",
    "print(\"方差为:\", variance)\n",
    "\n",
    "print(\"标准差为:\", standard_deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db120583",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfb6922",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e93ceef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8027804f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
